{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G-0ejhbMmnkuhSey6oW2om4DEBjHvwJq","timestamp":1741953619875}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/farrelrassya/teachingMLDL/blob/main/01.%20Machine%20Learning/02.%20Week%202/Notebook/RegresionBostonHouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZeKEGtkTP2Wu","outputId":"c13d254d-e924-4df6-8f92-91d4e710c70c","executionInfo":{"status":"error","timestamp":1741953659819,"user_tz":-420,"elapsed":2691,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c4e5b3e40dec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from sklearn.datasets import load_boston\n","data = load_boston()\n","X = data.data\n","y = data.target\n","\n","#`load_boston` has been removed from scikit-learn since version 1.2."]},{"cell_type":"code","source":["import pandas as pd\n","\n","url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n","df = pd.read_csv(url)\n","print(df.head())"],"metadata":{"id":"c0Ej5LS2WUZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"qD4UhG0zW0y_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corr = df.corr()\n","\n","# Bikin heatmap korelasi\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n","plt.title(\"Heatmap Korelasi\")\n","plt.show()"],"metadata":{"id":"GdvkS9tIQD6w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$$\n","r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n","$$"],"metadata":{"id":"AEtx0MfjQXQO"}},{"cell_type":"markdown","source":["Penjelasan Lengkap Persamaan Korelasi Pearson:\n","\n","1. Definisi:\n","   - $r$ adalah koefisien korelasi Pearson yang mengukur kekuatan dan arah hubungan linear antara dua variabel.\n","   - Nilai $r$ berkisar antara -1 (korelasi negatif sempurna) hingga 1 (korelasi positif sempurna), dengan $r = 0$ artinya tidak ada hubungan linear yang signifikan.\n","\n","2. Numerator:\n","   - Numerator, yaitu $\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$, merupakan jumlah dari produk deviasi setiap nilai $x_i$ dan $y_i$ terhadap rata-rata masing-masing, $\\bar{x}$ dan $\\bar{y}$.\n","   - Jika nilai $x_i$ dan $y_i$ sama-sama di atas atau di bawah rata-rata, produk deviasinya positif, menandakan hubungan positif. Sebaliknya, jika salah satunya di atas dan yang lainnya di bawah rata-rata, produk deviasinya negatif, menandakan hubungan negatif.\n","\n","3. Denumerator:\n","   - Denumerator adalah $\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}$.\n","   - Bagian ini menghitung total variasi masing-masing variabel dengan mengakumulasi kuadrat deviasi terhadap rata-rata, mengalikan kedua jumlah tersebut, lalu diakarkan. Ini menormalisasi nilai $r$, memastikan hasilnya selalu berada dalam rentang -1 sampai 1.\n","\n","4. Interpretasi:\n","   - $r = 1$: Hubungan linear positif sempurna, artinya kenaikan $x$ selalu diiringi kenaikan $y$ secara proporsional.\n","   - $r = -1$: Hubungan linear negatif sempurna, artinya kenaikan $x$ diiringi penurunan $y$ secara proporsional.\n","   - $r = 0$: Tidak ada hubungan linear yang signifikan antara variabel.\n","\n","5. Aplikasi:\n","   - Persamaan ini sering digunakan dalam statistik, machine learning, dan penelitian untuk mengevaluasi serta memahami hubungan antara variabel.\n","   - Normalisasi pada denumerator memastikan bahwa skala variabel tidak mempengaruhi besarnya koefisien korelasi.\n"],"metadata":{"id":"IzlNXEfWQsdx"}},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/farrelrassya/teachingMLDL/main/01.%20Machine%20Learning/02.%20Week%202/Picture/Linear_Correlation.png\" width=\"600\" height=\"600\">\n"],"metadata":{"id":"xtdkVyeBRgZd"}},{"cell_type":"code","source":["df.hist(bins=20, figsize=(9, 9))\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GZlkxDm-QAAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","print(\"Skewness tiap fitur:\")\n","print(df.skew())\n","\n","# Buat figure dengan grid 4x4\n","fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n","axes = axes.flatten()\n","\n","# Plot tiap fitur di subplot yang udah disediakan\n","for ax, col in zip(axes, df.columns):\n","    sns.histplot(df[col], kde=True, bins=20, ax=ax)\n","    ax.set_title(f'Distribusi {col}\\nSkewness: {df[col].skew():.2f}')\n","    ax.set_xlabel(col)\n","    ax.set_ylabel('Frequency')\n","\n","# Matikan subplot yang kosong kalo fitur kurang dari 16\n","if len(df.columns) < len(axes):\n","    for ax in axes[len(df.columns):]:\n","        ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ADL1o2KGW9F5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Contoh transformasi log untuk fitur dengan positive skew\n","df['crim_log'] = np.log1p(df['crim'])\n","df['zn_log']   = np.log1p(df['zn'])\n","df['chas_log'] = np.log1p(df['chas'])  # meskipun chas itu dummy, tapi nilainya skewed karena ketidakseimbangan\n","\n","# Untuk fitur dengan negative skew, lo bisa coba transformasi Yeo-Johnson\n","from scipy import stats\n","df['b_yj'], _ = stats.yeojohnson(df['b'])\n","\n","# Liat lagi skewnessnya setelah transformasi\n","print(\"Skewness setelah transformasi:\")\n","print(df[['crim_log', 'zn_log', 'chas_log', 'b_yj']].skew())"],"metadata":{"id":"PGvHAG2TXMRY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$$\n","y^{(\\lambda)} =\n","\\begin{cases}\n","\\displaystyle \\frac{(y+1)^\\lambda - 1}{\\lambda}, & \\text{if } y \\ge 0 \\text{ and } \\lambda \\neq 0, \\\\[10pt]\n","\\log(y+1), & \\text{if } y \\ge 0 \\text{ and } \\lambda = 0, \\\\[10pt]\n","-\\displaystyle \\frac{(-y+1)^{2-\\lambda} - 1}{2-\\lambda}, & \\text{if } y < 0 \\text{ and } \\lambda \\neq 2, \\\\[10pt]\n","-\\log(-y+1), & \\text{if } y < 0 \\text{ and } \\lambda = 2.\n","\\end{cases}\n","$$\n"],"metadata":{"id":"rf37kAaaYl5R"}},{"cell_type":"code","source":["fig, axes = plt.subplots(4, 4, figsize=(16, 8))  # 4x4 grid, total 16 subplot\n","axes = axes.flatten()\n","\n","# Looping buat plot tiap fitur\n","for i, feature in enumerate(transformed_features):\n","    sns.histplot(df[feature], kde=True, bins=20, ax=axes[i])\n","    axes[i].set_title(f'Distribusi {feature} setelah transformasi')\n","    axes[i].set_xlabel(feature)\n","    axes[i].set_ylabel('Frekuensi')\n","\n","# Hapus axes yang gak kepake (biar gak nampilin plot kosong)\n","for j in range(i + 1, 16):\n","    fig.delaxes(axes[j])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gmYMUQRkXtHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Copy dataset dan replace fitur asli dengan transformasi yang udah dibuat\n","df_model = df.copy()\n","df_model['crim'] = df_model['crim_log']\n","df_model['zn']   = df_model['zn_log']\n","df_model['chas'] = df_model['chas_log']\n","df_model['b']    = df_model['b_yj']\n","\n","# Misal targetnya tetap 'medv'\n","X = df_model.drop(['medv', 'crim_log', 'zn_log', 'chas_log', 'b_yj'], axis=1)\n","y = df_model['medv']\n","\n","# Split data: 80% training, 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Training model Linear Regression\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Prediksi dan evaluasi\n","y_pred = model.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Mean Squared Error:\", mse)"],"metadata":{"id":"RR5RxZXmZH-w","executionInfo":{"status":"error","timestamp":1741962677274,"user_tz":-420,"elapsed":9965,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"d822dbcd-5ca2-4a53-b51a-fbade5649e32"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4653d67dea1d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Copy dataset dan replace fitur asli dengan transformasi yang udah dibuat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crim'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crim_log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zn'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zn_log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Visualisasi Actual vs Predicted\n","plt.figure(figsize=(8, 6))\n","sns.scatterplot(x=y_test, y=y_pred)\n","plt.xlabel(\"Nilai Aktual MEDV\")\n","plt.ylabel(\"Nilai Prediksi MEDV\")\n","plt.title(\"Actual vs Predicted MEDV\")\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # line perfect prediction\n","plt.show()\n","\n","# Visualisasi distribusi residual\n","residuals = y_test - y_pred\n","plt.figure(figsize=(8, 6))\n","sns.histplot(residuals, kde=True, bins=20)\n","plt.xlabel(\"Residual\")\n","plt.title(\"Distribusi Residual\")\n","plt.show()"],"metadata":{"id":"zv6Q9MN2ZuH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question!\n","\n","1. Pada model di atas menggunakan transformasi data Yeo-Johnson. Eksplorasi transformasi data lainnya seperti Log Transform, Box-Cox Transform, atau Quantile Transform. Lakukan pembuatan model dengan masing-masing transformasi dan bandingkan hasilnya!\n","\n","  **Jawab :** Pada model regresi yang sebelumnya menggunakan transformasi data Yeo-Johnson, telah dilakukan eksplorasi terhadap beberapa transformasi data lainnya, yaitu Log Transform, Box-Cox Transform, dan Quantile Transform. Setelah menerapkan masing-masing transformasi dan mengevaluasi performa model menggunakan Mean Squared Error (MSE), ditemukan bahwa Log Transform memberikan hasil terbaik dengan MSE sebesar 18.47. Transformasi ini membantu mengurangi skewness pada data, sehingga meningkatkan kinerja model. Sementara itu, Yeo-Johnson dan Box-Cox menghasilkan MSE yang hampir sama, yaitu 19.69, mengindikasikan bahwa kedua metode ini memiliki efek yang serupa dalam mentransformasikan data. Adapun Quantile Transform memberikan MSE sebesar 19.20, yang sedikit lebih baik dibandingkan Yeo-Johnson dan Box-Cox, tetapi masih kalah dibandingkan Log Transform. Berdasarkan hasil ini, dapat disimpulkan bahwa Log Transform adalah pilihan terbaik dalam meningkatkan performa model regresi pada dataset ini.\n","\n","2. Lakukan eksplorasi terhadap metode normalisasi dan standardisasi data. Bandingkan metode MinMax Scaling, Standard Scaling, dan Robust Scaling dalam konteks data yang mengandung outlier. Bagaimana dampaknya terhadap akurasi model!\n","\n","  **Jawab :** Dalam analisis metode normalisasi dan standardisasi pada dataset yang mengandung outlier, terdapat tiga teknik utama yang dibandingkan: MinMax Scaling, Standard Scaling, dan Robust Scaling.\n","\n","  * MinMax Scaling mengubah nilai ke dalam rentang [0,1], tetapi sangat sensitif terhadap outlier karena nilai ekstrim mempengaruhi skala secara keseluruhan.\n","  * Standard Scaling menormalkan data dengan mengurangi mean dan membaginya dengan standar deviasi, menjadikannya kurang sensitif terhadap outlier dibandingkan MinMax Scaling, tetapi tetap dipengaruhi oleh distribusi data.\n","  * Robust Scaling menggunakan median dan interquartile range (IQR), menjadikannya paling tahan terhadap outlier karena tidak bergantung pada mean atau deviasi standar.\n","\n","  Dalam eksperimen menggunakan dataset Boston Housing, model regresi linear diuji setelah menerapkan setiap metode scaling. Hasil menunjukkan bahwa MinMax Scaling dapat menyebabkan distorsi karena outlier, Standard Scaling bekerja lebih baik tetapi masih dipengaruhi oleh distribusi data, sedangkan Robust Scaling memberikan akurasi yang lebih stabil karena ketahanannya terhadap outlier. Oleh karena itu, dalam dataset dengan outlier yang signifikan, Robust **Scaling** menjadi pilihan terbaik untuk meningkatkan kinerja model machine learning.\n","\n","3. Pada model sebelumnya, fitur numerik langsung digunakan tanpa encoding. Eksplorasi apakah melakukan discretization pada fitur numerik dapat meningkatkan performa model, dibandingkan dengan tetap menggunakan fitur dalam bentuk kontinu.\n","\n","  **Jawab :** Eksplorasi dilakukan untuk mengetahui apakah discretization pada fitur numerik dapat meningkatkan performa model regresi linear dibandingkan dengan menggunakan fitur dalam bentuk kontinu. Dataset Boston Housing digunakan, di mana fitur numerik awalnya dimanfaatkan dalam bentuk aslinya tanpa encoding. KBinsDiscretizer diterapkan untuk membagi fitur numerik menjadi 5 kategori, kemudian model regresi linear dilatih kembali dengan data yang telah didiscretisasi. Setelah membandingkan Mean Squared Error (MSE) antara kedua model, hasil menunjukkan bahwa model dengan fitur kontinu memiliki performa lebih baik dibandingkan dengan fitur yang telah didiscretisasi. Hal ini mengindikasikan bahwa dalam kasus ini, discretization dapat menghilangkan informasi penting yang dibutuhkan oleh model. Oleh karena itu, untuk dataset ini, lebih disarankan menggunakan fitur dalam bentuk kontinu, terutama karena regresi linear lebih efektif dalam menangani hubungan linier antar variabel."],"metadata":{"id":"MfxxIy9rTbww"}},{"cell_type":"markdown","source":["# Penjelasan Pertanyaan diatas dengan lebih mendetail\n","\n","---\n","\n","## 1. Transformasi Data\n","\n","**Deskripsi:**  \n","Pada model di atas digunakan transformasi data Yeo-Johnson.\n","\n","**Tugas:**  \n","Eksplorasi transformasi data alternatif seperti **Log Transform**, **Box-Cox Transform**, dan **Quantile Transform**.\n","\n","**Langkah Kerja:**  \n","- Lakukan pembuatan model dengan masing-masing metode transformasi.  \n","- Bandingkan hasil performa model yang dihasilkan dari tiap metode.\n","\n","---\n","\n","## 2. Normalisasi dan Standardisasi Data\n","\n","**Deskripsi:**  \n","Metode normalisasi dan standardisasi perlu dieksplorasi, terutama pada data yang mengandung outlier.\n","\n","**Tugas:**  \n","Bandingkan metode **MinMax Scaling**, **Standard Scaling**, dan **Robust Scaling**.\n","\n","**Langkah Kerja:**  \n","- Terapkan masing-masing metode pada dataset yang mengandung outlier.  \n","- Analisis dan evaluasi dampak dari masing-masing metode terhadap akurasi model.\n","\n","---\n","\n","## 3. Discretization pada Fitur Numerik\n","\n","**Deskripsi:**  \n","Pada model sebelumnya, fitur numerik digunakan dalam bentuk kontinu tanpa encoding.\n","\n","**Tugas:**  \n","Eksplorasi apakah melakukan **discretization** pada fitur numerik dapat meningkatkan performa model.\n","\n","**Langkah Kerja:**  \n","- Ubah fitur numerik menjadi bentuk kategori melalui teknik discretization.  \n","- Lakukan pelatihan model dengan fitur yang telah didiscretisasi.  \n","- Bandingkan performa model dengan model yang menggunakan fitur kontinu.\n","\n","---"],"metadata":{"id":"GWDtEzaGV0Ak"}}]}